# LLM Question-Answering Application ðŸ¥‹

This application utilizes Streamlit to create an interface for asking questions about the content of uploaded documents and receiving answers using Language Models (LLMs) and embeddings.


## Usage

1. Upload a file: Supported file formats include PDF, DOCX, and TXT.
2. Adjust the chunk size and the value of K as needed.
3. Click the "Add data" button to read, chunk, and embed the uploaded file.
4. Once the file is uploaded and processed, you can ask questions about its content in the provided text input field.
5. The application will display the answer generated by the Language Model (LLM) based on the provided question.
6. The chat history will be displayed below the input field, showing the questions asked and the corresponding answers.

## Functionality

- `load_document(file)`: Loads a document from the given file path.
- `chunk_data(data, chunk_size, chunk_overlap)`: Splits the document data into chunks.
- `create_embeddings(chunks)`: Creates embeddings for the chunks using OpenAI embeddings.
- `ask_and_get_answer(vector_store, q, k)`: Retrieves and returns answers for a given question using the provided vector store and value of K.
- `calculate_embedding_cost(texts)`: Calculates the cost of embedding the provided text data.
- `clear_history()`: Clears the chat history.

## Dependencies

- Streamlit
- Langchain
- OpenAI
- Python dotenv
